import json
import requests
import subprocess
from langchain_ollama import ChatOllama
from langchain_core.messages import SystemMessage, HumanMessage

class OllamaRunner:
    def __init__(self, model_name : str, personality : str, role : str, base_url="http://localhost:11434"):
        self.model_name = model_name
        self.base_url = base_url
        self.model_installed = False  # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìƒíƒœë¥¼ ì¶”ì í•˜ëŠ” ë³€ìˆ˜
        self.personality = personality
        self.role = role

    def is_model_installed(self):
        """í˜„ì¬ ì„¤ì¹˜ëœ Ollama ëª¨ë¸ ëª©ë¡ì„ í™•ì¸í•˜ì—¬ í•´ë‹¹ ëª¨ë¸ì´ ìˆëŠ”ì§€ ê²€ì‚¬"""
        try:
            result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
            return self.model_name in result.stdout
        except FileNotFoundError:
            print("âŒ Ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False

    def pull_model(self):
        """ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ (URLì—ì„œ ê°€ì ¸ì™€ì„œ ì„¤ì¹˜)"""
        if self.model_installed or self.is_model_installed():  
            self.model_installed = True
            return True
        url = f"{self.base_url}/api/pull"
        response = requests.post(url, json={"name": self.model_name})

        if response.status_code == 200:
            self.model_installed = True
            return True
        else:
            print(f"âš ï¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {response.text}")
            return False
        
    # def run_model_interactive(self):
    #     """Ollama ëª¨ë¸ì„ í„°ë¯¸ë„ì—ì„œ ì§ì ‘ ì‹¤í–‰ ('ollama run <model>')"""
    #     if not self.pull_model():
    #         print("âŒ ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨!")
    #         return

    #     print(f"ğŸš€ '{self.model_name}' ëª¨ë¸ì„ ì‹¤í–‰ ì¤‘... ")
    #     subprocess.run(["ollama", "run", self.model_name])

    def generate_text(self, prompt : str, temperature : float, max_tokens : int) -> str:
        """í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ë©´ Ollama ëª¨ë¸ì´ ì‘ë‹µ"""
        if not self.pull_model():
            print("âŒ ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨!")
            return "Error: Model could not be loaded"

        url = f"{self.base_url}/api/generate"
        payload = {"model": self.model_name, "prompt": prompt}

        with requests.post(url, json=payload, stream=True) as response:
            if response.status_code != 200:
                return f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {response.text}"

            generated_text = ""
            for line in response.iter_lines():
                if line:
                    try:
                        data = json.loads(line)  # ì—¬ê¸°ì„œ ë³€ê²½
                        if "response" in data:
                            generated_text += data["response"] + " "
                    except json.JSONDecodeError:
                        continue  # JSON íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¬´ì‹œ

            return generated_text.strip()
        
    def generate_text_with_vectorstore(self, user_prompt: str, vectorstore, k: int = 3, max_tokens: int = 100) -> str:
        """
        ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰í•œ í›„, ì´ë¥¼ í¬í•¨í•˜ì—¬ Ollama ëª¨ë¸ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        :param user_prompt: ì‚¬ìš©ì ì…ë ¥ í”„ë¡¬í”„íŠ¸
        :param vectorstore: FAISS ë“± ë²¡í„°ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤
        :param k: ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œ ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ê°’: 3)
        :param max_tokens: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (Ollamaì—ì„œ í•´ë‹¹ ì˜µì…˜ì´ ì§€ì›ë˜ëŠ” ê²½ìš° í™œìš© ê°€ëŠ¥)
        :return: ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìƒíƒœë¥¼ í™•ì¸í•˜ê³  í•„ìš” ì‹œ ë‹¤ìš´ë¡œë“œ
        if not self.model_installed:
            if not self.pull_model():
                print("âŒ ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨!")
                return "Error: Model could not be loaded"

        try:
            # ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
            search_results = vectorstore.similarity_search(user_prompt, k=k)
            context = "\n".join([doc.page_content for doc in search_results])
        except Exception as e:
            context = ""
            print(f"ë²¡í„°ìŠ¤í† ì–´ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

        # Ollamaì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ ìƒì„±
        full_prompt = f"Context:\n{context}\n\nUser: {user_prompt}"

        # Ollama API í˜¸ì¶œ (generate_text ì´ìš©)
        response = self.generate_text(full_prompt)
        return response
