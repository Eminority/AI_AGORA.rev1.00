import requests
import concurrent.futures
from bs4 import BeautifulSoup
from duckduckgo_search import DDGS
from time import time

# âœ… í¬ë¡¤ë§ì´ ë¶ˆê°€ëŠ¥í•œ ì‚¬ì´íŠ¸ ë¦¬ìŠ¤íŠ¸ (ì—¬ê¸°ì— ì¶”ê°€ ê°€ëŠ¥)
blocked_sites = set(["msn.com", "www.fnnews.com", "www.hani.co.kr", "www.moneycontrol.com", "www.mcknights.com", 
                     "abcnews.go.com", "www.thedailybeast.com", "www.msnbc.com", "www.business-standard.com",
                     "www.reuters.com", "www.washingtonpost.com", "www.nationalreview.com", "www.newsweek.com"])

time_1 = time()
def search_news_articles(query, max_search=1000, max_results=20):
    """
    ë•ë•ê³ ì—ì„œ ë‰´ìŠ¤ ê²€ìƒ‰ í›„ í¬ë¡¤ë§ ë¶ˆê°€ëŠ¥í•œ ì‚¬ì´íŠ¸ë¥¼ ì œì™¸í•œ URL ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    """
    with DDGS() as ddgs:
        results = list(ddgs.news(query, max_results=max_search))

    news_data = []
    for result in results:
        domain = result["url"].split("/")[2]  # URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ

        # âœ… í¬ë¡¤ë§ ë¶ˆê°€ëŠ¥í•œ ì‚¬ì´íŠ¸ ìë™ í•„í„°ë§
        if any(site in domain for site in blocked_sites):
            print(f"ğŸš« í•„í„°ë§ëœ ì‚¬ì´íŠ¸: {domain} (í¬ë¡¤ë§ ì°¨ë‹¨ë¨)")
            continue  

        news_data.append({
            "title": result["title"],
            "url": result["url"],
            "domain": domain
        })

        # âœ… ìµœëŒ€ í¬ë¡¤ë§í•  ê¸°ì‚¬ ê°œìˆ˜ ë„ë‹¬ ì‹œ ì¤‘ë‹¨
        if len(news_data) >= max_results:
            break

    return news_data

def crawl_news_article(url):
    """
    ë‰´ìŠ¤ ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ì„ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜ (ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥)
    """
    try:
        response = requests.get(url, timeout=5)

        if response.status_code != 200:
            return {"url": url, "content": f"âŒ ìš”ì²­ ì‹¤íŒ¨ (Status Code: {response.status_code})"}

        soup = BeautifulSoup(response.text, "html.parser")

        # âœ… ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì˜ ë³¸ë¬¸ ì„ íƒì§€
        possible_selectors = [
            "article", "div.article-body", "div.story-body",
            "div.post-content", "div.entry-content", "div.main-content",
            "section.article", "div#main-content", "div.text", "div.content",
            "div[class*='article']", "div[class*='content']", "div[class*='body']", 
            "p"
        ]

        article_text = ""
        for selector in possible_selectors:
            elements = soup.select(selector)
            for elem in elements:
                paragraphs = elem.find_all("p")
                if paragraphs:
                    article_text += "\n".join([p.get_text() for p in paragraphs]) + "\n\n"

        if not article_text.strip():
            # âœ… í¬ë¡¤ë§ ì‹¤íŒ¨í•œ ì‚¬ì´íŠ¸ë¥¼ ì°¨ë‹¨ ëª©ë¡ì— ì¶”ê°€
            domain = url.split("/")[2]
            if domain not in blocked_sites:
                blocked_sites.add(domain)
                print(f"âš ï¸ í¬ë¡¤ë§ ë¶ˆê°€ ì‚¬ì´íŠ¸ ì¶”ê°€ë¨: {domain}")

            return {"url": url, "content": "âŒ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (HTML êµ¬ì¡° í™•ì¸ í•„ìš”)"}

        return {"url": url, "content": article_text.strip()}

    except Exception as e:
        return {"url": url, "content": f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}

def parallel_crawl(news_list, max_workers=10):
    """
    ë³‘ë ¬ë¡œ ì—¬ëŸ¬ ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ì„ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜
    """
    final_results = []

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # âœ… ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰ (ê° URL í¬ë¡¤ë§)
        future_to_url = {executor.submit(crawl_news_article, news["url"]): news for news in news_list}

        for future in concurrent.futures.as_completed(future_to_url):
            result = future.result()
            final_results.append(result)

            # âœ… í¬ë¡¤ë§ ê²°ê³¼ ì¶œë ¥
            print(f"\nğŸ“„ ê¸°ì‚¬ ë³¸ë¬¸ (URL: {result['url']}):")
            print(result["content"][:500])  # ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ ì¶œë ¥
            print("=" * 80)

    return final_results

# âœ… ë‰´ìŠ¤ ê²€ìƒ‰ ë° ë³‘ë ¬ í¬ë¡¤ë§ ì‹¤í–‰
search_query = "Is it beneficial to walk your pet?"
max_articles = 20  # ìµœì¢… í¬ë¡¤ë§í•  ê¸°ì‚¬ ê°œìˆ˜

print(f"\n=== ğŸ” '{search_query}' ê²€ìƒ‰ ì¤‘ (ìµœëŒ€ {max_articles}ê°œ ê¸°ì‚¬) ===\n")
news_results = search_news_articles(search_query, max_search=1000, max_results=max_articles)

# âœ… ë³‘ë ¬ í¬ë¡¤ë§ ì‹¤í–‰
print("\nğŸš€ ë³‘ë ¬ í¬ë¡¤ë§ ì‹œì‘...")
final_articles = parallel_crawl(news_results, max_workers=10)

print(f"\nâœ… {max_articles}ê°œ ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ!")
time_2 = time()
print(f"ê±¸ë¦° ì‹œê°„ : {time_2-time_1:.2f}")